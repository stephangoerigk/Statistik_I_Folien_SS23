<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Einheit 1</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Stephan Goerigk" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#EE0071"],"pen_size":4,"eraser_size":40,"palette":[]}) })</script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




name: Title slide
class: middle, left
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
# Statistik I

### Einheit 5: Wahrscheinlichkeitstheorie und Verteilungen
##### 10.05.2023 | Prof. Dr. phil. Stephan Goerigk

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen 

#### Wiederholung:

**Inferenzstatistik: **

Umfasst alle statistischen Verfahren, die es erlauben, trotz der Informationsunvollständigkeit der Stichprobendaten Aussagen über eine Population zu treffen.

**Population: **

* Gesamtheit aller Merkmalsträger:innen, auf die eine Untersuchungsfrage gerichtet ist.

**Stichprobe: **

* Auswahl bestimmter Merkmalsträger:innen aus einer Population

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen 

#### Wiederholung:

**Problem:**

* Wenn nur ein Teil der Grundgesamtheit erfasst wird, z.B. 100 Personen, ist die **Informationslage** in Bezug auf die Untersuchungsfrage **unvollständig**. Wir können nicht einfach deskriptiv-statistische Methoden verwenden.

* Wie kann man trotzdem Aussagen treffen, die sich auf alle Personen der Grundgesamtheit beziehen, obwohl nur die Daten einer Stichprobe vorliegen?

**Idee:**

* Wir ziehen die Personen zufällig aus der Population in die Stichprobe.

* Wir greifen auf mathematische Methoden zur Formalisierung von Zufallsprozessen zurück `\(\rightarrow\)` Wahrscheinlichkeitstheorie

* Aus diesen ergeben sich Methoden, die Rückschlüsse von der Stichprobe auf die Population erlauben `\(\rightarrow\)` Inferenzstatistik

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen 

#### Logik des Schließens von Stichprobe auf Population (Einzelschritte folgen)

.center[
&lt;img src="bilder/Population.png" width="450px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Inferenzstatistik:**

* Schluss von Zufallsstichprobe auf Population

* Grundlage: Wahrscheinlichkeitsrechnung

* Zentral: Zufallsprozesse (Ausgang unsicher, nicht mit Sicherheit vorhersagbar)

**Wahrscheinlichkeitsrechnung:**

.center[*Mathematik ist der Versuch, alles zu bändigen, auch den Zufall.*

Rudolf Taschner]

* Statistischer Wahrscheinlichkeitsbegriff geht zurück auf 17. Jahrhundert (Frankreich)

* Im Jahr 1654 wandte sich der Glücksspieler Chevalier de Mere mit mehreren Fragen an den französischen Mathematiker Blaise Pascal

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Stochastik:**

* Stochastik = die Kunst des Vermutens (altgriechisch)

* Mathematik setzt Vorstellung von Zufall voraus (= Modelle von Situationen, deren Ausgang unsicher ist)

* Keine Einzelereignisse vorhersagbar, aber:

* Erkennen von Regelmäßigkeiten bei Vorgängen, deren Ergebnisse vom Zufall abhängen.

* Zentraler Begriff: Zufallsexperiment

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsexperiment:**

Im Prinzip beliebig oft wiederholbarer Vorgang, der nach bestimmter Vorschrift ausgeführt wird, wobei das Ergebnis vom Zufall abhängt, d.h. der Ausgang kann nicht eindeutig im voraus bestimmt werden.

* Folge von gleichartigen, voneinander unabhängigen Versuchen möglich.

* Entweder Folge voneinander unabhängiger Versuche mit einem Objekt oder jeweils einmaliger Versuche mit ”gleichartigen” (unabhängigen) Objekten.

Beispiele:

1. Ein Würfel wird wiederholte Male geworfen und es wird beobachtet, wie oft jede Zahl kommt.

2. Parteipräferenz bei weiblichen Jugendlichen zwischen 16 und 18 Jahren.

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsexperiment - Nomenklatur:**

* Die möglichen Ergebnisse eines Zufallsexperimentes heißen Elementarereignisse ω

* Die Menge aller möglichen Ergebnisse eines Zufallsexperimentes bezeichnet man als Ereignisraum Ω.

* Beispiel: ’Einmaliges Würfeln’: Elementarereignisse sind {1}, {2}, {3}, {4}, {5}, {6}. Ereignisraum `\(Ω = {1, 2, 3, 4, 5, 6}\)`.

* Ereignis A: Teilmenge des Ereignisraums, z.B. alle geraden Augenzahlen beim Würfeln. Es gilt: `\(ω ∈ A, A ⊂ Ω\)`

* Sicheres Ereignis: Jenes Ereignis, welches unter gegebenen Bedingungen immer eintritt.

* Unmögliches Ereignis: Jenes Ereignis, welches unter gegebenen Bedingungen nie eintritt.

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Definition der statistischen Wahrscheinlichkeit:**

Die Wahrscheinlichkeit für das Auftreten eines Ereignisses A, `\(P_{(A)}\)`, ist jener Wert, bei dem sich die relative Häufigkeit `\(r_{n}(A)\)` bei
n `\(\rightarrow\)` ∞ Versuchen unter gleichen Bedingungen stabilisiert.

Die mathematische Formulierung:

.center[
&lt;img src="bilder/stat_Wahrscheinlichkeit.png" width="250px" /&gt;
]

In anderen Worten:

* Die Wahrscheinlichkeit eines Ereignisses gibt an, mit welcher relativen Häufigkeit das Ereignis einträte, wenn man den Versuch theoretisch unendlich oft wiederholen würde.

* Sie sagt jedoch nichts darüber aus, wie häufig das Ereignis bei einer kleinen Anzahl von Versuchen, z.B. `\(n = 5\)`, auftritt.

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Laplace-Wahrscheinlichkeit**


Bei Zufallsexperimenten, bei denen nur endlich viele, gleichwahrscheinliche Ergebnisse möglich sind, ergibt sich für ein beliebiges Ereignis A die Wahrscheinlichkeit

.center[
&lt;img src="bilder/Laplace.png" width="750px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Axiome der Wahrscheinlichkeitsrechnung nach Kolmogoroff**

Wahrscheinlichkeiten lassen sich durch drei Eigenschaften, die auch f̈ür relative Häufigkeiten gelten, und aus denen sich alle Rechenregeln für Wahrscheinlichkeiten ableiten lassen, charakterisieren:


1. Für die Wahrscheinlichkeit eines Ereignisses gilt stets: `\(0 ≤ P(A) ≤ 1\)`.

2. Die Wahrscheinlichkeit eines sicheren Ereignisses beträgt `\(P(Ω) = 1\)`.

3. Additionsregel der Wahrscheinlichkeit: Die Wahrscheinlichkeit, dass eines von `\(k\)` einander ausschließenden Ereignissen auftritt, ist die Summe der einzelnen Wahrscheinlichkeiten `\(P(A_{1}), P(A_{2}), . . . , P(A_{k})\)`.

`$$P(A_{1} ∨ A_{2} ∨...∨ A_{k})=P(A_{1})+P(A_{2})+...+P(A_{k})$$`
---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Rechenregeln: Unmögliches Ereignis**

* Die Wahrscheinlichkeit des unmöglichen Ereignisses B beträgt `\(P(B) = 0\)`.
 
* Wenn B ein unmögliches Ereignis ist, kann es nie eintreten: `\(rn(B) = 0 → P(B) = 0\)`.

Achtung: 

* Aus `\(P(B) = 0\)` folgt aber nicht, dass B ein unmögliches Ereignis ist. 

* Das bedeutet nur, dass der Grenzwert der relativen Häufigkeit bei `\(n → ∞\)` Null ist, woraus aber nicht folgt, dass B nie eintreten kann! (Analoges gilt für `\(P(A) = 1\)`).

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Rechenregeln: Komplementärereignis**

* `\(P(A)+P(\bar{A})=1,P(\bar{A})=1−P(A)\)`

* `\(\bar{A}\)` tritt immer dann ein, wenn `\(A\)` nicht eintritt → `\(r_{n}(A) + r_{n}(\bar{A}) = 1\)`

Beispiel: Münzwurf: `\(P(K) + P(Z) = 0.5 + 0.5 = 1\)`

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Bedingte Wahrscheinlichkeit**

* Bedingte Wahrscheinlichkeit P(A|B) (wörtlich: A unter der Bedingung B)

* Die Wahrscheinlichkeit von Ereignis A unter der Bedingung, dass Ereignis B bereits eingetreten ist. 

* D.h. das Eintreten von B beeinflusst die Wahrscheinlichkeit von A.

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Beispiel: Bedingte Wahrscheinlichkeit**

* 100 Patient:innen, die vor einiger Zeit auf einer Warteliste für eine Psychotherapie standen, werden nach ihrem subjektiven Gesundheitszustand befragt.

* Einige dieser Patient:innen haben zwischenzeitlich einen Therapieplatz bekommen, andere noch nicht.

Bivariate Häufigkeitstabelle: Häufigkeiten von bereits in Behandlung befindlichen und noch wartenden KlientInnen bezüglich subjektiv gesund und nicht gesund.

.center[
&lt;img src="bilder/bedingte_wahrscheinlichkeit1.png" width="550px" /&gt;
]

Frage: Wie hoch ist die Wahrscheinlichkeit P(G|T), dass eine Patient:in, die Therapie erhält, gesund ist?

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Beispiel: Bedingte Wahrscheinlichkeit**

.center[
&lt;img src="bilder/bedingte_wahrscheinlichkeit2.png" width="550px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Multiplikationssatz der Wahrscheinlichkeit**

* Durch Umformen der bedingten Wahrscheinlichkeit ergibt sich `\(P(G ∧ T) = P(G|T)P(T)\)`

* Aus den bedingten Wahrscheinlichkeiten kann man die Wahrscheinlichkeit des Durchschnittsereignisses ausrechnen.

Beispiel:

* Mit einer Wahrscheinlichkeit von 1% rollt einem Autofahrer in einer Wohngegend ein Ball vor das Auto. 
* Die Wahrscheinlichkeit dafür, dass unmittelbar hinter dem Ball ein Kind nachgelaufen kommt, beträgt p = 0.99. 
* Wie groß ist die Wahrscheinlichkeit, dass ein Ball auf die Straße rollt und ein Kind hinterherläuft?

`$$P(B ∧ K) = P(K|B)P(B) = 0.99 · 0.01 = 0.0099$$`
---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Stochastische Unabhängigkeit von Ereignissen**

Zwei Ereignisse sind stochastisch unabhängig, wenn das Eintreten des einen Ereignisses keinen Einfluss auf das Eintreten des anderen Ereignisses hat.

`$$P(A|B) = P(A)$$` und `$$P(B|A) = P(B)$$`

Zwei Ereignisse sind voneinander unabhängig, wenn die Wahrscheinlichkeit für das gemeinsame Auftreten dem Produkt ihrer Einzelwahrscheinlichkeiten entspricht (Multiplikationssatz für unabhängige Ereignisse)

`$$P(A|B)= \frac{P(A∧B)}{P(B)} = P(A)⇒P(A∧B)=P(A)P(B)$$`
---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Stochastische Unabhängigkeit von Ereignissen**

* Beim Ziehen mit Zurücklegen sind die einzelnen Wahrscheinlichkeiten gleich und die Ziehungen stochastisch unabhängig.

* Beim Ziehen ohne Zurücklegen ändern sich mit jeder Ziehung die Anteile der ’günstigen’ ωi , und daher auch die Wahrscheinlichkeiten. Die Ziehungen sind daher stochastisch abhängig.

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsexperiment**

* Jedes mögliche Ergebnis aus einem Zufallsexperiment nennen wir ein Elementarereignis ω

* Die Menge aller möglichen Ereignisse ist definiert als der Ereignisraum Ω

* Der Ereignisraum Ω heißt diskret, wenn er aus abzählbar vielen Elementarereignissen besteht

* Der Ereignisraum Ω heißt stetig, wenn er aus überabzählbar vielen Elementarereignissen besteht

* Zufallsexperiment ist ein allgemeiner Begriff, der Grundlage für die Inferenzstatistik ist

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsexperiment**

* Zufallsexperiment 1: 

  Einmaliger Wurf mit einer Münze; Ω = { Kopf, Zahl } endlich und abzählbar

* Zufallsexperiment 2: 

   Wurf mit einem Würfel so lange bis zum ersten Mal drei ’Einser’ hintereinander kommen; wir interessieren uns für die Anzahl der notwendigen Würfe; Ω = 3,4,5,··· oder Ω = {k : k natürliche Zahl ≥ 3}; Ω ist diskret und abzählbar unendlich

* Zufallsexperiment 3: 

  Lebensdauer einer Glühbirne; Ω = {x : x ≥ 0}; Ω ist stetig und überabzählbar unendlich

* Richtige Bestimmung von Ω ist Voraussetzung für Richtigkeit jeder weiteren statistischen Analyse

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsvariable**

* Es seien ein Wahrscheinlichkeitsraum Ω und p(ω) für alle ω gegeben

* Eine mathematische Funktion X, welche jedem Ereignis ω eine reelle Zahl X(ω) zuweist, heißt Zufallsvariable (ZV)

* X ist eine Zufallsvariable (ZV), wenn die Werte von X reelle Zahlen sind, die durch ein Zufallsexperiment bestimmt werden, und wenn für die Ereignisse, die man damit beschreiben kann, Wahrscheinlichkeiten angebbar sind

* Der Wert, den die ZV bei der Durchführung des Zufallsexperimentes annimmt, heißt Realisation von X, und wird mit x bezeichnet

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsvariable**

* Zufallsvariable lässt sich durch ihre Wahrscheinlichkeitsfunktion beschreiben, welche angibt, mit welcher Wahrscheinlichkeit die einzelnen Realisationen `\(x_{i}\)` auftreten

* Es sei `\(p_{i}\)` die Wahrscheinlichkeit des Auftretens des Wertes `\(x_{i}\)`; dann ist

`$$f(x_{i})=P(X=x_{i})=p_{i}; p_{i} ∈[0,1]$$`

* Wenn alle möglichen Ausprägungen von X berücksichtigt wurden,ist die Summe aller möglichen Einzelwahrscheinlichkeiten `\(p_{i}\)` = 1

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Dichtefunktion**

* Eine stetige ZV `\(X\)` kann jeden Wert in einem Intervall [a, b] annehmen

* Die Wahrscheinlichkeiten der einzelnen Ausprägungen (Werte) einer stetigen ZV können (im Gegensatz zum diskreten Fall) nicht angegeben werden

* Es können nur Wahrscheinlichkeiten `\(f(x)dx\)` angegeben werden, mit welchen die Werte innerhalb von Intervallen `\(dx\)` um die Werte `\(x\)` auftreten

* Beispielsweise fragt man nicht, wie viele Personen exakt 1.75 Meter groß sind, sondern z.B., wie viele Personen zwischen 1.75 und 1.76 Meter groß sind

* Die Funktion `\(f(x)\)` heißt Dichtefunktion

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Dichtefunktion**

* Die Wahrscheinlichkeit, dass die ZV Werte zwischen a und b annimmt, wird dann allgemein definiert als das Integral über die Dichtefunktion mit Integrationsgrenzen a und b.

* Analog zum diskreten Fall erhält man durch Integration die Verteilungsfunktion

* Die Wahrscheinlichkeit ist definiert als Fläche unter der Dichtefunktion

.center[
&lt;img src="bilder/dichte1.png" width="350px" /&gt;
]

Es gilt für alle `\(a&lt;b\)`:

.center[
&lt;img src="bilder/dichte2.png" width="550px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Erwartungswert**

Beispiel: X ist die erhaltene Augenzahl bei einmaligem Würfeln; die Wahrscheinlichkeitsverteilung von X ist:

.center[
&lt;img src="bilder/erwartungswert1.png" width="350px" /&gt;
]
* Welchen Wert ’erwarten’ wir, wenn wir dieses Zufallsexperiment sehr lange durchführen?

* Intuitiv erwarten wir `\(X = 1\)` bei `\(\frac{1}{6}\)` der Würfe, `\(X = 2\)` bei  `\(\frac{1}{6}\)` bei der Würfe, usw.

* Der Durchschnitt von X auf lange Sicht ist der Erwartungswert von X

* Der Erwartungswert einer ZV ist ein Maß für das Zentrum der Verteilung

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Varianz der ZV**

Die Varianz `\(σ^2\)` ist ein Streuungsmaß der Verteilung

`$$σ_{X}^2 =E[(X−E[X])^2]=E[X^2]−(E[X])^2$$`

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Varianz der ZV**

Beispiel: X ist die beobachtete Augenzahl bei einmaligem Würfeln; die Wahrscheinlichkeitsverteilung von X ist

.center[
&lt;img src="bilder/varianz.png" width="650px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**α-Quantil**

Als α-Quantil `\(q_{α}\)` wird ein Wert bezeichnet, unterhalb dessen ein vorgegebener Anteil `\(α\)` aller Fälle der Verteilung liegen

* Jeder Wert unterhalb von `\(q_{α}\)` unterschreitet den Anteil `\(α\)`, mit `\(α\)` als reelle Zahl zwischen 0 (gar kein Fall der Verteilung) und 1 (alle Fälle oder 100% der Verteilung)

* Für stetige ZV gilt:

.center[
&lt;img src="bilder/alpha_quantil.png" width="450px" /&gt;
]

* Für diskrete ZV gilt (Aufrunden zur nächsten ganzzahligen Ausprägung):

.center[
&lt;img src="bilder/alpha_quantil2.png" width="450px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Diskrete Gleichverteilung**

* Diese Verteilung beschreibt eine ZV, welche die Zahlen `\(1,2,··· ,m\)` annehmen kann, und es gilt:

.center[
&lt;img src="bilder/gleichverteilung1.png" width="450px" /&gt;
]

* Anwendung bei Zufallsexperimenten, deren Ergebnisse gleich häufig sind, also wenn angenommen wird, dass die `\(m\)` Elementarereignisse gleichwahrscheinlich sind

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Diskrete Gleichverteilung**

.center[
&lt;img src="bilder/gleichverteilung2.png" width="650px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Diskrete Gleichverteilung**

* Beispiel: X = die erhaltene Augenzahl bei einmaligem Würfeln

.center[
&lt;img src="bilder/gleichverteilung3.png" width="250px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Diskrete Gleichverteilung**

.center[
&lt;img src="Statistik_I_5_files/figure-html/unnamed-chunk-15-1.png" width="350px" height="350px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Binomialverteilung**

* Wir betrachten ein Zufallsexperiment mit 2 Ausgängen, ’Erfolg (2)’ und ’Misserfolg (1)’

* Die Wahrscheinlichkeit für Erfolg sei `\(p\)`, mit `\(p\)` zwischen 0 und 1

* Wir führen dieses Experiment n-mal durch, wobei zwischen den einzelnen Durchführungen Unabhängigkeit angenommen wird (’Ziehen mit Zurücklegen’)

* Die ZV X beschreibt die Anzahl der Erfolge und ist binomialverteilt mit Parametern n und p, X ~ `\(B(n, p)\)`

.center[
&lt;img src="bilder/binomial1.png" width="550px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Binomialverteilung**

.center[
&lt;img src="Statistik_I_5_files/figure-html/unnamed-chunk-17-1.png" width="350px" height="350px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Binomialverteilung**

* Beispiel: Ein Glücksrad besteht aus 20 Feldern, wobei 5 davon Gewinnfelder sind. 

* Wie groß ist die Wahrscheinlichkeit, dass Sie zwei Mal gewinnen, wenn Sie das Glücksrad drei Mal drehen?

* Experiment mit 2 Ausgängen, Erfolg (5 Gewinnfelder) und Misserfolg

* n = 3, weil wir das Glücksrad drei Mal drehen

* `\(p = \frac{5}{20} = 0.25\)` ist die Wahrscheinlichkeit zum Erfolg

.center[
&lt;img src="bilder/binomial2.png" width="550px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Binomialverteilung**

* Binomialverteilte ZV nimmt Werte zwischen 0 und n an

* Binomialverteilung ist symmetrisch für p = 0.5

* Je kleiner/größer p desto rechts/links-schiefer die Verteilung

* Summe mehrerer Bernoulli-Variablen

Erwartungswert und Varianz:

`$$E[X]=np$$` 

`$$σ^2 =np(1−p)$$`

* Für `\(n = 1\)`: `\(B(1, p)\)` ist eine Bernoulli-ZV mit Erwartungswert p und Varianz `\(p(1 − p)\)`

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Poisson-Verteilung**

* Diese Verteilung beschreibt ZV, die alle natürliche Zahlen und 0 annehmen können

Wahrscheinlichkeitsfunktion:

.center[
&lt;img src="bilder/poisson1.png" width="550px" /&gt;
]

* λ ist der Parameter der Poisson-Verteilung und kann jede reelle positive Zahl sein

Erwartungswert und Varianz:

`$$E[X]=λ$$`

`$$σ^2=λ$$`
---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Poisson-Verteilung**

* Poisson-Verteilung ist Grenzverteilung der Binomialverteilung bei `\(n\)` → `\(∞\)` und `\(p\)` → 0 unter der Nebenbedingung, dass `\(np = λ\)` beschränkt bleibt

* Poisson-Verteilung kann als gute Approximation für die Binomialverteilung bei großem n und kleinem p verwendet werden

* Poisson-Verteilung beschreibt seltene Ereignisse

* Anwendung bei binomialverteilter ZV mit unbekanntem oder großem `\(n\)` (leichtere Berechnung) und kleinem `\(p\)`

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Beispiel: Poisson-Verteilung**

Die Wahrscheinlichkeit, dass ein Patient die Injektion eines Serums nicht verträgt sei 0.001. Wie groß ist die Wahrscheinlichkeit, dass von 200 Patienten mehr als 1 die Injektion nicht vertragen?

.center[
&lt;img src="bilder/poisson2.png" width="550px" /&gt;
]

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Normalverteilung (NV)**

* Die NV ist eine stetige Verteilung, die durch 2 Parameter `\(μ\)` und `\(σ\)` charakterisiert ist

* Es sei X eine ZV die `\(N(μ,σ^2)\)` verteilt ist; X kann Werte zwischen `\(−∞\)` und `\(+∞\)` annehmen

Dichtefunktion φ(x):

.center[
&lt;img src="bilder/nv1.png" width="350px" /&gt;
]

* Geht x → `\(±∞\)` strebt `\(φ(x)\)` gegen 0

* `\(φ(x)\)` ist symmetrisch um `\(μ\)`

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Normalverteilung (NV)**

* `\(σ\)` gibt den Abstand zwischen `\(μ\)` und den Wendepunkten der Dichtefunktion an

* Wendepunkte an den Stellen `\(μ±σ\)`

* Wenn `\(σ\)` groß ist, ist die Verteilung breit und niedrig, wenn `\(σ\)` klein ist, ist die Verteilung schmal und hoch

* Fläche unter `\(φ(x)\)` zwischen `\(−∞\)` und `\(+∞\)` ist gleich 1

* Die Fläche `\(μ ± σ\)` umfasst ca. 68% aller Fälle

* Die Fläche `\(μ ± 2σ\)` umfasst ca. 95% aller Fälle

* Es existieren unendlich viele NV durch beliebige Auswahl von `\(μ\)` und `\(σ\)`


---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Normalverteilung (NV)**

.center[
&lt;img src="Statistik_I_5_files/figure-html/unnamed-chunk-22-1.png" width="350px" height="350px" /&gt;
]


---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Standardnormalverteilung `\(N(0,1)\)`**

* Spezielle NV für `\(μ = 0\)` und `\(σ = 1\)` (Gauß’sche Glockenkurve)

* Verteilung der `\(N(0,1)\)` ist tabelliert

* Fläche zwischen `\(μ = 0\)` und einem beliebigen Wert z ist ablesbar 

* Quantile der NV; 1-Fläche rechts von einem Wert z, und links von −z 

* Ist X `\(N(μ,σ^2)\)` verteilt dann führt die Transformation `\(\frac{X−μ}{σ}\)` auf eine `\(N(0,1)\)` Verteilung

* Vorteil, da Quantile in Tabellen ablesbar (es müssen nicht jedes mal Integrale für Dichtefunktion berechnet werden)

---
class: top, left
### Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Standardnormalverteilung `\(N(0,1)\)`**

.center[
&lt;img src="Statistik_I_5_files/figure-html/unnamed-chunk-23-1.png" width="350px" height="350px" /&gt;
]

---
class: top, left
### Take-aways

.full-width[.content-box-gray[
* **Inferenzstatistik** ist ein wahrscheinlichkeitsbasierter Schluss von Zufallsstichprobe auf Population

* Variablen in der Population sind nicht vollständig beobachtbar und daher **Zufallsvariablen** (diskret vs. stetig)

* **Wahrscheinlichkeitsfunktion** definiert welche Werte wir beim zufälligen Ziehen mit welcher Wahrscheinlichkeit erwarten

* Der **Erwartungswert** ist das Zentrum der Verteilung und der wahrscheinlichste Wert 

* Unter der **Gleichverteilung** ist jedes Ereignis gleich wahrscheinlich

* **Binomialverteilung** lässt uns Wahrscheinlichkeit für ein diskretes Ereignis mit 2 Ausgängen berechnen

* **Poisson Verteilung** ist die Grenzfunktion der Bionmialverteilung für besonders seltene Ereignisse

* **Normalverteilung** ist stetige Verteilung, die extremen Ereignissen geringere und durchschnittlichen Ereignissen höhere Wahrscheinlichkeit zuweist 
]
]

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
